{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision and Deep Learning \n",
    "## Lab 2.a - Mono Camera Calibration Example\n",
    "This lab looks into calibrating camera systems. To this end we want to utilise corresponding points within a set of images to help identify intrinsic parameters linking the cameras in the real-world.\n",
    "\n",
    "There are two ways to approach this lab. You can either grab the calibration board from the front and grab a series of images from the KinectV2 using code similar to the first lab, or you can utilise the data I have captured myself. Both will then be used in the same way later on, but it may be nice to have a bit of personalisation for the data you will be using.\n",
    "\n",
    "First we need to obtain some images, identify a checkerboard pattern on the RGB image maps and then register the points identifed on the checkerboard across the different images. \n",
    "\n",
    "We will utilize OpenCV functionality to undertake a large portion of the pipeline, however I will also give a brief insight into the more hands-on approach that we can take (but this will be in MATLAB). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a id=\"imports\"></a>\n",
    "The following section defines the imports used for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ce74a4d705a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# For image processing applications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# For saving images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# For ndarray handling:\n",
    "import numpy as np\n",
    "\n",
    "# For plotting:\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "plt.rcParams['figure.figsize'] = [9, 10]\n",
    "\n",
    "# For TCP/IP functionality:\n",
    "from struct import unpack\n",
    "import socket\n",
    "\n",
    "# For image processing applications\n",
    "import cv2\n",
    "\n",
    "# For saving images\n",
    "from PIL import Image\n",
    "\n",
    "# For Operating System tools and file finding\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab calibration images.\n",
    "We can either grab some calibration images from the KinectV2 in a similar fashion to last lab, or we can simply load some example data from disk. Change the `capture_new` flag to switch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = 0 # Edit this to select your number of requested frames\n",
    "img = []\n",
    "dep = []\n",
    "infra = []\n",
    "\n",
    "capture_new = False\n",
    "\n",
    "# A function to get our messages over TCP/IP\n",
    "def getMessage(sock, data_type):\n",
    "    msg_size = sock.recv(8)\n",
    "    (msg_size,) = unpack('>Q', msg_size)\n",
    "    data = b''\n",
    "    while len(data) < msg_size:\n",
    "        left_to_read = msg_size - len(data)\n",
    "        data += sock.recv(8192 if left_to_read > 8192 else left_to_read)\n",
    "    data = np.frombuffer(data, data_type)\n",
    "    sock.send(b'1')\n",
    "    return data\n",
    "\n",
    "\n",
    "# Get images from either disk or over TCP/IP\n",
    "if capture_new: # Get data over TCP/IP\n",
    "    server_ip = '137.44.6.201' # Edit this to reflect the correct IP address\n",
    "    server_port = 20156\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    s.connect((server_ip, server_port))\n",
    "    \n",
    "    request_frame = True\n",
    "    while request_frame:\n",
    "        print('Getting frame: {0}'.format(n_frames))\n",
    "        s.send(bytes([1]))\n",
    "\n",
    "        # Get color\n",
    "        img.append(getMessage(s, np.uint8))\n",
    "        # Get depth\n",
    "        dep.append(getMessage(s, np.uint16))\n",
    "        # Get infrared\n",
    "        infra.append(getMessage(s, np.uint8))\n",
    "\n",
    "        print('Frame {0} done'.format(n_frames))\n",
    "        n_frames += 1\n",
    "        \n",
    "        print('Type q and press enter to stop collecting frames:')\n",
    "        if input() == 'q':\n",
    "            request_frame = False\n",
    "\n",
    "    print('Finished grabbing data from server.')\n",
    "    s.close()\n",
    "    \n",
    "else: # Get data from disk\n",
    "    path_to_data = os.getcwd() # Edit this if data is another directory\n",
    "    data_files = glob.glob(path_to_data + '\\KinectV2_Image_frame*.npy')\n",
    "    n_frames = np.max([int(elem[-6:-4]) for elem in data_files]) + 1\n",
    "    \n",
    "    for i_frame in range(n_frames):\n",
    "        print('Getting frame: {0}'.format(i_frame))\n",
    "        img.append(np.load('KinectV2_Image_frame{0:02d}.npy'.format(i_frame)))\n",
    "        dep.append(np.load('KinectV2_Depth_frame{0:02d}.npy'.format(i_frame)))\n",
    "        infra.append(np.load('KinectV2_Infra_frame{0:02d}.npy'.format(i_frame)))\n",
    "    \n",
    "    print('Finished grabbing data from disk.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape the data packets into 2D maps\n",
    "The byte streams coming in over the TCP/IP connection are a flattened form of the color, depth and infrared maps captured by the KinectV2. We therefore want to reshape these vectors into their original shapes. For the color maps we require a `Height` $\\times$ `Width` $\\times$ `Channel` ndarray, depth and infrared maps both require a `Height` $\\times$ `Width` ndarray.\n",
    "\n",
    "The native KinectV2 color map resolution is `Height`: 1920, `Width`: 1080 and `Channel`: 4 (BGRA - Blue, Green, Red, Alpha), but the depth and infrared maps are `Height`: 424, `Width`: 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the frames\n",
    "IMGW = 1920  # Image width\n",
    "IMGH = 1080  # Image Height\n",
    "DEPW = 512\n",
    "DEPH = 424\n",
    "INFRAW = DEPW\n",
    "INFRAH = DEPH\n",
    "img = [np.reshape(elem,(IMGH, IMGW, -1)) for elem in img]\n",
    "dep = [np.reshape(elem,(DEPH, DEPW)) for elem in dep]\n",
    "infra = [np.reshape(elem,(INFRAH, INFRAW)) for elem in infra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the new captures if needed.\n",
    "Save both the numpy array information, and the appearance pngs for later if wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = False\n",
    "if save_data:\n",
    "    for i_frame in range(n_frames):\n",
    "        np.save('KinectV2_Image_frame{0:02d}.npy'.format(i_frame), img[i_frame])\n",
    "        np.save('KinectV2_Depth_frame{0:02d}.npy'.format(i_frame), dep[i_frame])\n",
    "        np.save('KinectV2_Infra_frame{0:02d}.npy'.format(i_frame), infra[i_frame])\n",
    "        \n",
    "        im = Image.fromarray(img[i_frame][:, :, [2, 1, 0]])\n",
    "        im.save(\"KinectV2_Image_frame{0:02d}.png\".format(i_frame))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color channel correction\n",
    "Notice that the Kinect returns the color channels in BGRA order. For `matplotlib`'s image plotting functionality we will want to pass the channels in RGB order, for this we slice into our color map and reorder the channel dimension with `[:, :, [2, 1, 0]]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_frame in range(n_frames):\n",
    "    img[i_frame] = img[i_frame][:, :, [2, 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot frames\n",
    "In this following section we want to plot the color, depth and infrared maps we have collected from the KinectV2. \n",
    "\n",
    "Note that these maps do not match eachother exactly, pixels within the color map do not correspond directly to the corresponding pixel in the depth and infrared maps. The sensors on the KinectV2 are mounted at opposite ends of the sensor bar, and have differing resolutions and fields of view. \n",
    "\n",
    "This difference in viewpoint means that in order to align (or more formally ''register'') the color to the depth map we need to calculate the intrinsic and extrinsic parameters of the numerous Kinect sensors, we will cover this in the coming labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over our captured frames, and subplot each map \n",
    "for i_frame in range(n_frames):\n",
    "    fig = plt.figure(figsize=[9, 2])\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(img[i_frame])\n",
    "    plt.axis('off')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(dep[i_frame])\n",
    "    plt.axis('off')\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(infra[i_frame])\n",
    "    plt.axis('off')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect the checkerboard pattern in an image. \n",
    "Using OpenCV, we can use the `findChessboardCorners()` method to identify a standard checkerboard pattern, commonly by identification of Harris corners. This method takes in the orignal image in grayscale, which we use `cvtColor` to achieve, and a tuple of the internal corner counts `checkerboard_pattern`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkerboard_pattern = (9, 6) # This is the internal corner count\n",
    "plot = True # toggle to control plotting of images\n",
    "\n",
    "# Detect checkerboard in each image\n",
    "img_corners = []\n",
    "for i_frame in range(n_frames):\n",
    "    \n",
    "    # Detect corners in the RGB image\n",
    "    image = cv2.cvtColor(img[i_frame], cv2.COLOR_BGR2GRAY) # image needs to be grayscale\n",
    "    img_pattern_found, img_corner = cv2.findChessboardCorners(image, checkerboard_pattern, None) # Locate pattern in image\n",
    "    \n",
    "    if img_pattern_found:\n",
    "        # Probably don't need to refine, but if needed then uncomment this section\n",
    "        #subpixel_criteria =  (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001) # Stopping criteria for refinement\n",
    "        #img_corner = cv2.cornerSubPix(image, img_corner, (11, 11), (-1, -1), subpixel_criteria)\n",
    "        \n",
    "        # Add detected corners into list for later\n",
    "        img_corners.append(img_corner)\n",
    "        \n",
    "        if plot: \n",
    "            # plot corners over image\n",
    "            plt.figure()\n",
    "            plt.imshow(image, cmap='gray')\n",
    "            corners = np.squeeze(img_corner)\n",
    "            plt.scatter(corners[:, 0], corners[:, 1], s=3)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "n_boards = len(img_corners)\n",
    "print('Number of successfully found patterns is: {0} out of {1}'.format(n_boards, n_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrate camera using detected patterns\n",
    "The following cell looks to calibrate the camera using using the identified corners from the patterns on the calibration board in each image.\n",
    "\n",
    "First we build a coordinate system for the pattern, this requires specifying the 3D points of the corners on the board from the board's world coordinate system. For example: if we have a board of size (3, 2) we would specify corners at: \n",
    "    [[0, 0], [0, 1], [0, 2], \n",
    "     [1, 0], [1, 1], [1, 2]]\n",
    "\n",
    "Next we use OpenCV functionality to calibrate the camera with the `calibrateCamera()` function. This returns the intrinsic camera matrix, `cam-mat`, the distortion coefficients for our camera, `distor`, and the rotation and translation vectors for each board `r_vec` and `t_vec`.\n",
    "\n",
    "We then create a dictionary, `camera_ex`, containing the extrinsic parameters for each set of points found. This contains the rotation matrix and translation vector. Note here that we turn the rotation vectors `r_vec` into the rotation matrices `rot_mat` using the `Rodrigues()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the coordinate points of our checkerboard pattern (meshgrid makes this easier)\n",
    "squareH = 40\n",
    "squareW = squareH\n",
    "board = np.zeros((np.product(checkerboard_pattern), 3), dtype=np.float32)\n",
    "board[:, :2] = np.mgrid[0:squareH*(checkerboard_pattern[0] - 1):complex(checkerboard_pattern[0]),\n",
    "                                 0:squareW*(checkerboard_pattern[1] - 1):complex(checkerboard_pattern[1])].T.reshape(-1, 2)\n",
    "\n",
    "#Create a list of the checkerboard patterns\n",
    "boards = [board for i in range(n_boards)]\n",
    "\n",
    "# Calibrate camera\n",
    "ret, cam_mat, distor, r_vec, t_vec = cv2.calibrateCamera(boards, img_corners, image.shape[::-1], None, None)\n",
    "\n",
    "# Set up intrinsics and extrinsic parameters\n",
    "camera_in = cam_mat\n",
    "camera_ex = []\n",
    "for i_board in range(n_boards):\n",
    "    camera_ex.append({})\n",
    "    camera_ex[i_board] = {'rot_mat': np.asarray(cv2.Rodrigues(r_vec[i_board])[0]),\n",
    "                          'tran_mat' : np.asarray(t_vec[i_board])}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate boards to world coordinate system and plot camera centric viewpoint\n",
    "In this cell we will transform the board coordinate system into the world coordinate system of the camera. We can then plot these points in the 3D space relative to our camera centred at (0, 0, 0). This makes the assumption that our camera is fixed and we move our checkerboard pattern around.\n",
    "\n",
    "First we create our 3D matplotlib figure and plot the camera origin. We also plot 3 lines denoting the axes for this camera system.\n",
    "\n",
    "Next we loop over our identified board patterns and translate them into the camera's world coordinate system via the rotation and translation identified via calibration.\n",
    "\n",
    "Finally, we print these new checkerboard pattern points in the 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up a 3D figure\n",
    "fig3 = plt.figure(figsize=[6, 6])\n",
    "ax3 = fig3.add_subplot(111, projection='3d')\n",
    "ax3.scatter(0, 0, 0) # plot our camera at (0, 0, 0)\n",
    "ax3.plot(np.asarray([0,  0, 0, 64, 0,   0]), # Plot 3 lines showing the 3 camera axes (X, Y, Z)\n",
    "         np.asarray([0, 64, 0,  0, 0,   0]), \n",
    "         np.asarray([0,  0, 0,  0, 0, 128]))\n",
    "\n",
    "# Perform transforms and plot for each board\n",
    "world_coord_boards = []\n",
    "for i_board in range(n_boards):\n",
    "    # Obtain the rotation matrices and translation vectors from the extrinsic parameters for each board\n",
    "    R = camera_ex[i_board]['rot_mat']\n",
    "    T = np.squeeze(camera_ex[i_board]['tran_mat'])\n",
    "    \n",
    "    # Perform the rotation and translation of the boards relative to the camera centred at origin: Rx+T\n",
    "    corners_world = np.matmul(R, board.T).T\n",
    "    corners_world += T\n",
    "    world_coord_boards.append(corners_world)\n",
    "    \n",
    "    # Plot our board corners in 3D\n",
    "    ax3.scatter(xs=world_coord_boards[-1][:, 0], ys=world_coord_boards[-1][:, 1], zs=world_coord_boards[-1][:, 2])\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    ax3.set_zlabel('z')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate cameras and plot the pattern centric viewpoints\n",
    "In this cell we will transform the camera coordinate system into the world coordinate system of the board. We can then plot these points in the 3D space relative to our board centred at (0, 0, 0). This makes the assumption that our pattern is fixed and we move our camera around.\n",
    "\n",
    "First we create our 3D matplotlib figure and plot the board pattern at origin. We also plot 3 lines denoting the axes for this board system.\n",
    "\n",
    "Next we loop over each camera viewpoint in our identified board patterns and translate them into the board's world coordinate system via the inversed of the translation and rotation identified via calibration.\n",
    "\n",
    "Finally, we print these new camera positions and orientations in the 3D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open up a 3D figure\n",
    "fig3 = plt.figure(figsize=[6, 6])\n",
    "ax3 = fig3.add_subplot(111, projection='3d')\n",
    "ax3.scatter(board[:, 1], board[:, 0], board[:, 2])# plot our board at (0, 0, 0)\n",
    "ax3.plot(np.asarray([0,  0, 0, 64, 0,   0]), # Plot 3 lines showing the 3 board axes (X, Y, Z)\n",
    "         np.asarray([0, 64, 0,  0, 0,   0]), \n",
    "         np.asarray([0,  0, 0,  0, 0, 128]),'r')\n",
    "\n",
    "# Create coordinates for camera axes plots, these will be translated and rotated into the board-centric world\n",
    "cam_axes = np.asarray([[0,  0, 0, 64, 0,  0],\n",
    "                       [0, 64, 0,  0, 0,  0],\n",
    "                       [0,  0, 0,  0, 0, 64]]).T\n",
    "\n",
    "# Perform transforms and plot for each camera\n",
    "camera_positions = []\n",
    "for i_camera in range(n_boards): # R'(x-T)\n",
    "    # Obtain the rotation matrices and translation vectors from the extrinsic parameters for each board\n",
    "    R = camera_ex[i_camera]['rot_mat']\n",
    "    T = np.squeeze(camera_ex[i_camera]['tran_mat'])\n",
    "    \n",
    "    # Perform the rotation and translation of the boards relative to the camera centred at origin: R'(x-T)\n",
    "    new_cam_axes = cam_axes - T\n",
    "    new_cam_axes = np.matmul(R.T, new_cam_axes.T).T\n",
    "    camera_positions.append(new_cam_axes)\n",
    "\n",
    "    # Plot cameras\n",
    "    ax3.scatter(xs=camera_positions[-1][:, 0], ys=camera_positions[-1][:, 1], zs=camera_positions[-1][:, 2], s=2)\n",
    "    ax3.plot(camera_positions[-1][:, 0], camera_positions[-1][:, 1], camera_positions[-1][:, 2])\n",
    "    ax3.set_xlabel('x')\n",
    "    ax3.set_ylabel('y')\n",
    "    ax3.set_zlabel('z')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
